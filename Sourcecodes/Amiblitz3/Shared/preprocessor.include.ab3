; XTRA
; Embedded .xtra Header
; 
; General Info
; -------------------------------------------------------
; ExePath         = "///System"
; ExeFile         = ""
; CreateIcon      = 0
; Residents       = "all.res"
; 
; Compiler
; -------------------------------------------------------
; StringBuffer    = 4096
; MakeSmallest    = 1
; Version         = 0.0.0
; NumberOfBuilds  = 88
; 
; Debugger
; -------------------------------------------------------
; CliArgs         = ""
; StackSize       = 4095
; RuntimeDebug    = 0
; DebugInfo       = 1
; OverflowCheck   = 0
; AssemblerCheck  = 0
; InterruptCheck  = 1
; AutoRun         = 1
; 
; Editor
; -------------------------------------------------------
; CursorLine      = 320
; CursorColumn    = 2
; LabelSearch     = "writede"
; 
; Blitz Objects
; -------------------------------------------------------
; Max IconInfo    = 1
; Max NChunky     = 50
; Max MUIObject   = 50
; Max PTModule    = 5
; Max AsyncReq    = 4
; Max Req-Lib     = 5
; Max GTMenuList  = 25
; Max Console     = 5
; Max TCPSock     = 5
; Max XBSound     = 10
; Max Chunky      = 20
; Max File        = 25
; Max GadgetList  = 5
; Max Queue       = 10
; Max Screen      = 5
; Max Shape       = 100
; Max CopList     = 10
; Max Sprite      = 20
; Max Stencil     = 5
; Max Module      = 5
; Max Window      = 25
; Max Anim        = 1
; Max Sound       = 1
; Max Bank        = 5
; Max Buffer      = 10
; Max BitMap      = 10
; Max Slice       = 10
; Max Page        = 1
; Max Tape        = 5
; Max IntuiFont   = 5
; Max MedModule   = 8
; Max Palette     = 4
; Max MenuList    = 5
; Max BlitzFont   = 4
; Max GTList      = 25
; /XTRA
optimize 5
Syntax 2

XINCLUDE "hashmap.include.bb2"
CNIF #__include=0
XINCLUDE "common.bb2"
CEND

NEWTYPE.preprocessor
*tokens.hashmap
tokenFlags.w[256]
*detokens.l
End NEWTYPE

#TOKENF_IS        = $01 ;/* is part of token */
#TOKENF_EOL       = $02 ;/* end of line */
#TOKENF_NEW       = $04 ;/* starts a possibly new token */
#TOKENF_STOP      = $08 ;/* stops a token */
#TOKENF_QUOT      = $10 ;/* start/end quoting */
#TOKENF_COMMENT   = $20 ;/* comment */
#TOKENF_COPY      = $40 ;/* copy over to destination */
#TOKENF_FIRST     = $80 ;/* can serve as first char in token */


Statement preproc_FlushTokens{*tn.preprocessor}
If *tn=!_NULL Then Statement Return
If *tn\tokens   Then hashmap_Free{*tn\tokens} : *tn\tokens   = !_NULL
If *tn\detokens Then FreeVec_ *tn\detokens    : *tn\detokens = !_NULL
End Statement


Statement preproc_Free{*tn.preprocessor}
If *tn=!_NULL Then Statement Return
preproc_FlushTokens{*tn}
FreeVec_ *tn : *tn=!_NULL
End Statement


Statement preproc_Update{*tn.preprocessor}
If *tn=!_NULL Then Statement Return
If *tn\detokens=!_NULL
  *tn\detokens = AllocVec_ (128*256*4,#MEMF_CLEAR)
Else
  For n.l=0 To 128*256-1
    Poke.l *tn\detokens + n*4,!_NULL
  Next
End If

*hm.hashmap = *tn\tokens
*ta.l       = *tn\detokens
If *ta><!_NULL AND *hm><!_NULL
  For n.l=0 To *hm\size-1
    ptr.l   = Peek.l(*hm\base + n*4)
    If ptr
      size.l   = Peek.l(ptr) : ptr +4
      endptr.l = ptr + size
      While ptr<endptr
        ikl.l = Peek.l(ptr) : ptr +4+ikl
        isl.l = Peek.l(ptr) : ptr +4+isl
        tokenIndex.l = (Peek.w(ptr-isl) & $7FFF)
        Poke.l *ta+tokenIndex*4,ptr-isl-4-ikl-4
      Wend
    End If
  Next
End If
End Statement


Function.l preproc_Create{}
*tn.preprocessor = AllocVec_ (SizeOf.preprocessor,#MEMF_CLEAR)
If *tn
  For n.l=0 To 255
    *tn\tokenFlags[n] = #TOKENF_STOP|#TOKENF_NEW
  Next
  *tn\tokenFlags[   0] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_EOL ; Null
  *tn\tokenFlags[   1] = #TOKENF_STOP|#TOKENF_NEW ; SOH
  *tn\tokenFlags[   2] = #TOKENF_STOP|#TOKENF_NEW ; STX
  *tn\tokenFlags[   3] = #TOKENF_STOP|#TOKENF_NEW ; ETX
  *tn\tokenFlags[   4] = #TOKENF_STOP|#TOKENF_NEW ; EOT
  *tn\tokenFlags[   5] = #TOKENF_STOP|#TOKENF_NEW ; ENQ
  *tn\tokenFlags[   6] = #TOKENF_STOP|#TOKENF_NEW ; ACK
  *tn\tokenFlags[   7] = #TOKENF_STOP|#TOKENF_NEW ; BELL
  *tn\tokenFlags[   8] = #TOKENF_STOP|#TOKENF_NEW ; BS  (backspace)
  *tn\tokenFlags[   9] = #TOKENF_STOP|#TOKENF_NEW ; TAB (tabulator)
  *tn\tokenFlags[  10] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_EOL ; LF  (line feed) Linux Return
  *tn\tokenFlags[  11] = #TOKENF_STOP|#TOKENF_NEW ; VT
  *tn\tokenFlags[  12] = #TOKENF_STOP|#TOKENF_NEW ; FF  (feed forward)
  *tn\tokenFlags[  13] = #TOKENF_STOP|#TOKENF_NEW ; CR  (carrier return) Windows Return
  *tn\tokenFlags[  14] = #TOKENF_STOP|#TOKENF_NEW ; SO
  *tn\tokenFlags[  15] = #TOKENF_STOP|#TOKENF_NEW ; SI
  *tn\tokenFlags[  16] = #TOKENF_STOP|#TOKENF_NEW ; DLE
  *tn\tokenFlags[  17] = #TOKENF_STOP|#TOKENF_NEW ; DC1
  *tn\tokenFlags[  18] = #TOKENF_STOP|#TOKENF_NEW ; CD2
  *tn\tokenFlags[  19] = #TOKENF_STOP|#TOKENF_NEW ; CD3
  *tn\tokenFlags[  20] = #TOKENF_STOP|#TOKENF_NEW ; CD4
  *tn\tokenFlags[  21] = #TOKENF_STOP|#TOKENF_NEW ; NAK
  *tn\tokenFlags[  22] = #TOKENF_STOP|#TOKENF_NEW ; SYN
  *tn\tokenFlags[  23] = #TOKENF_STOP|#TOKENF_NEW ; ETB
  *tn\tokenFlags[  24] = #TOKENF_STOP|#TOKENF_NEW ; CAN (Cancel)
  *tn\tokenFlags[  25] = #TOKENF_STOP|#TOKENF_NEW ; EM
  *tn\tokenFlags[  26] = #TOKENF_STOP|#TOKENF_NEW ; SUB
  *tn\tokenFlags[  27] = #TOKENF_STOP|#TOKENF_NEW ; ESC (Escape)
  *tn\tokenFlags[  28] = #TOKENF_STOP|#TOKENF_NEW ; FS
  *tn\tokenFlags[  29] = #TOKENF_STOP|#TOKENF_NEW ; GS
  *tn\tokenFlags[  30] = #TOKENF_STOP|#TOKENF_NEW ; RS
  *tn\tokenFlags[  31] = #TOKENF_STOP|#TOKENF_NEW ; US
  *tn\tokenFlags[@" "] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY ; Space
  *tn\tokenFlags[@"!"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[  34] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY |#TOKENF_QUOT
  *tn\tokenFlags[@"#"] = #TOKENF_STOP|#TOKENF_COPY
  *tn\tokenFlags[@"$"] = #TOKENF_STOP|#TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"%"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"&"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"'"] = #TOKENF_STOP|#TOKENF_COPY
  *tn\tokenFlags[@"("] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@")"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"*"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"+"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@","] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"-"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"."] = #TOKENF_STOP|#TOKENF_COPY
  *tn\tokenFlags[@"/"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"0"] = #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"1"] = #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"2"] = #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"3"] = #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"4"] = #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"5"] = #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"6"] = #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"7"] = #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"8"] = #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"9"] = #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@":"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@";"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COMMENT|#TOKENF_COPY
  *tn\tokenFlags[@"<"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"="] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@">"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"?"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"@"] = #TOKENF_STOP|#TOKENF_COPY
  *tn\tokenFlags[@"A"] = #TOKENF_FIRST|#TOKENF_IS  |#TOKENF_COPY
  *tn\tokenFlags[@"B"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"C"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"D"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"E"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"F"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"G"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"H"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"I"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"J"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"K"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"L"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"M"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"N"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"O"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"P"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"Q"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"R"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"S"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"T"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"U"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"V"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"W"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"X"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"Y"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"Z"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"["] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"\"] = #TOKENF_STOP|#TOKENF_COPY
  *tn\tokenFlags[@"]"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"^"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"_"] = #TOKENF_IS              |#TOKENF_COPY
  *tn\tokenFlags[@"`"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"a"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"b"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"c"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"d"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"e"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"f"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"g"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"h"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"i"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"j"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"k"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"l"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"m"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"n"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"o"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"p"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"q"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"r"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"s"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"t"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"u"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"v"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"w"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"x"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"y"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"z"] = #TOKENF_FIRST| #TOKENF_IS|#TOKENF_COPY
  *tn\tokenFlags[@"{"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"|"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"}"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[@"~"] = #TOKENF_STOP|#TOKENF_NEW|#TOKENF_COPY
  *tn\tokenFlags[ 127] = #TOKENF_STOP|#TOKENF_NEW
  *tn\tokens   = !_NULL
  *tn\detokens = !_NULL
End If
Function Return *tn
End Function


Function.l preproc_LoadTokens{*tn.preprocessor,hashmapfile.s}
If *tn = !_NULL Then Function Return !_NULL
*hm.hashmap = hashmap_Load{hashmapfile}
If *hm
  If *tn\tokens Then hashmap_Free{*tn\tokens} : *tn\tokens = !_NULL
  *tn\tokens = *hm
  preproc_Update{*tn}
End If
Function Return *hm
End Function


Function.w preproc_SaveTokens{*tn.preprocessor,hashmapfile.s}
If *tn = !_NULL Then Function Return False
If *tn\tokens
  Function Return hashmap_Save{*tn\tokens,hashmapfile}
End If
Function Return False
End Function


XINCLUDE "error.include.bb2"
; process the ascii data in sourcebuff length and write the tokenized source into destbuffer
; the return value is the length of the tokenized text
; sourcebuff and destbuff might be the same pointer
Function.l preproc_Tokenize{*tn.preprocessor,sourcebuff.l,destbuff.l,bytelength.l}
If *tn = !_NULL Then Function Return -1
If *tn\tokens = !_NULL Then Function Return -1
dptr.l        = destbuff
eptr.l        = sourcebuff+bytelength
sptr.l        = sourcebuff
tokenlength.l = 0
accept.l      = True
hash.l        = 0
;error{"Tokenizer..."}
While sptr<eptr
  c.b  = Peek.b(sptr)
  If c<0 Then c=32
  cf.l = *tn\tokenFlags[c&$FF]

  If tokenlength ; we have a token
    If (cf&#TOKENF_STOP) ; and a stop character
      If c=@"$"
        Poke.b dptr,c
        xhash.l = (hash+c)*196314165
        tptr.l = hashmap_GetItemByHash{*tn\tokens,dptr-tokenlength,tokenlength+1,xhash}
        If tptr=!_NULL
          tptr.l = hashmap_GetItemByHash{*tn\tokens,dptr-tokenlength,tokenlength,hash}
        Else
          cf=0
        End If
      Else
        tptr.l = hashmap_GetItemByHash{*tn\tokens,dptr-tokenlength,tokenlength,hash}
      End If
      If tptr ; we found it!
        dptr-tokenlength
        Poke.w dptr,Peek.w(tptr)
        dptr+2
        tokenlength = 0
        accept      = False
      End If
    End If

    If (cf&#TOKENF_IS) ; it is a valid token char
      If tokenlength ; we have a token!
        tokenlength+1
        hash = (hash+c)*196314165
      End If
    Else ; no, it's not a token
      tokenlength=0
    End If
  Else
    If (cf&#TOKENF_FIRST)
      If accept ; do we accept new tokens?
        tokenlength=1
        hash = c*196314165
      End If
    End If
  End If

  If (cf&(#TOKENF_QUOT|#TOKENF_COMMENT|#TOKENF_EOL))
    If (cf&#TOKENF_QUOT) ; it's a string constant
      Repeat
        Poke.b dptr,c : dptr+1
        sptr+1
        c  = Peek.b(sptr)
        cf = *tn\tokenFlags[c&$FF]
      Until ((cf&(#TOKENF_QUOT|#TOKENF_EOL)) OR (sptr>=eptr))
    End If

    If (cf&#TOKENF_COMMENT) ; it's a comment
      Repeat
        Poke.b dptr,c : dptr+1
        sptr+1
        c  = Peek.b(sptr)
        cf = *tn\tokenFlags[c&$FF]
      Until ((cf&(#TOKENF_EOL)) OR (sptr>=eptr))
    End If

    If (cf&#TOKENF_EOL     ) Then Poke.b dptr,0 : dptr+1

  End If

  If (cf&#TOKENF_NEW     ) Then accept=True:tokenlength=0:Else accept=False
  If (cf&#TOKENF_COPY    ) Then Poke.b dptr,c : dptr+1

  sptr+1
Wend
;error{"Tokenizer done"}
;Poke.b dptr,0
Function Return dptr-destbuff
End Function


Function.l preproc_DetokenizeFP{*tn.preprocessor,sourcebuff.l,bytelength.l,fp.l}
If *tn = !_NULL Then Function Return -1
If *tn\detokens = !_NULL Then Function Return -1
bptr.l        = AllocVec_(8192*2,#MEMF_ANY)
If bptr=!_NULL Then Function Return -1
dptr.l        = bptr                     ; destination pointer
aptr.l        = bptr + 8192              ; write if destpointer>aptr!
eptr.l        = sourcebuff+bytelength    ; end source pointer
sptr.l        = sourcebuff               ; source pointer
count.l       = 0
While sptr<eptr
  c.b  = Peek.b(sptr)

  If (c&$80)
    ; token...
    tokenIndex.l = Peek.w(sptr) & $7FFF
    tokenNode.l = Peek.l(*tn\detokens + tokenIndex*4)
    If tokenNode
      tokenLength.l = Peek.l(tokenNode) : tokenNode+4
      If tokenLength<8192
        For n.l=0 To tokenLength-1
          Poke.b dptr,Peek.b(tokenNode) : dptr+1 : tokenNode+1
        Next
      Else
        ; token is longer than 8192 chars, WTF!?
      End If
    Else
      libid.l   = (tokenIndex LSR 7) & $FF
      tokenid.l = tokenIndex & $7F
      defToken.s = "????Lib"+Str$(libid)+"/"+Str$(tokenid)
      For n.l=0 To FLen(defToken)-1
        Poke.b dptr,Peek.b(&defToken+n) : dptr+1
      Next
    End If
    sptr+2
  Else
    If c=0   ; it's a newline
      Poke.b dptr,$0A : dptr+1 : sptr+1
    Else
      Poke.b dptr,c : dptr+1 : sptr+1
    End If

    If (c=$22) ; it's a string!
      Repeat
        c  = Peek.b(sptr)
        Poke.b dptr,c : dptr+1
        sptr+1
      Until ((c=$22) OR (c=0) OR (sptr>=eptr))
      If c=0 Then Poke.b dptr-1,$0A
    End If

    If (c=@";") ; it's a comment!
      Repeat
        c  = Peek.b(sptr)
        Poke.b dptr,c : dptr+1
        sptr+1
      Until ((c=0) OR (sptr>=eptr))
      If c=0 Then Poke.b dptr-1,$0A
    End If
  End If

  If dptr>aptr
    wlen.l = dptr-bptr
    Write_ fp,bptr,wlen
    count + wlen
    dptr = bptr
  End If

Wend

If dptr>bptr
  wlen.l = dptr-bptr
  Write_ fp,bptr,wlen
  count + wlen
  dptr = bptr
End If

FreeVec_ bptr

Function Return count
End Function




Function.l preproc_Detokenize{*tn.preprocessor,sourcebuff.l,destbuff.l,bytelength.l}
If *tn = !_NULL Then Function Return -1
If *tn\detokens = !_NULL Then Function Return -1
dptr.l        = destbuff
eptr.l        = sourcebuff+bytelength
sptr.l        = sourcebuff
While sptr<eptr
  c.b  = Peek.b(sptr)

  If (c&$80)
    ; token...
    tokenIndex.l = Peek.w(sptr) & $7FFF
    tokenNode.l = Peek.l(*tn\detokens + tokenIndex*4)
    If tokenNode
      tokenLength.l = Peek.l(tokenNode) : tokenNode+4
      For n.l=0 To tokenLength-1
        Poke.b dptr,Peek.b(tokenNode) : dptr+1 : tokenNode+1
      Next
    End If
    sptr+2
  Else
    If c=0
      Poke.b dptr,$0A : dptr+1 : sptr+1
    Else
      Poke.b dptr,c : dptr+1 : sptr+1
    End If
    If (c=$22)
      Repeat
        c  = Peek.b(sptr)
        Poke.b dptr,c : dptr+1
        sptr+1
      Until ((c=$22) OR (c=0) OR (sptr>=eptr))
      If c=0 Then Poke.b dptr-1,$0A
    End If

    If (c=@";")
      Repeat
        c  = Peek.b(sptr)
        Poke.b dptr,c : dptr+1
        sptr+1
      Until ((c=0) OR (sptr>=eptr))
      If c=0 Then Poke.b dptr-1,$0A
    End If

  End If
Wend
Function Return dptr-destbuff
End Function


Function.l preproc_AddToken{*tn.preprocessor,tokenName.s,tokenID.l,libID.l}
If *tn = !_NULL Then Function Return !_NULL
If *tn\tokens = !_NULL Then *tn\tokens=hashmap_Create{14}
If *tn\tokens = !_NULL Then Function Return !_NULL
tokenW.w = libID LSL 7 | tokenID | $8000
tokenS.s = Peeks$(&tokenW,2)
Function Return hashmap_AddItem{*tn\tokens,tokenName,tokenS}
End Function



Statement preproc_QueryTokens{*PreProcessor.preprocessor}
preproc_FlushTokens{*PreProcessor}
!ComData_GetL{comPtr_TokenBase,D0}
*token.tokeninfo = PutD0

While *token
  name.s    =  Peek.s(&*token\dat)
  libid.l   = (*token\number LSR 7) &$FF
  tokenid.l = *token\number & $7F
  tokenS.s  = hashmap_GetItem{*PreProcessor\tokens,name}
  If tokenS
    dtoken.l = Peek.w(&tokenS)
    dtokenid.l = (dtoken) & $7F
    dlibid.l   = (dtoken LSR 7) &$FF
    ;error{"Token Clash: \\22"+name+"\\22 was found as "+Str$(libid)+"/" +Str$(tokenid)+" and before as "+Str$(dlibid)+"/"+Str$(dtokenid)+"!"}
  Else
    preproc_AddToken{*PreProcessor,name,tokenid,libid}
  End If
  *token = *token\next_token
Wend
preproc_Update{*PreProcessor}
End Statement






CNIF #__include=0
XINCLUDE "file.include.bb2"
XINCLUDE "eclock.include.bb2"
NPrint "Creating tokenizer..."

*tn.preprocessor = preproc_Create{}
If *tn
  If preproc_LoadTokens{*tn,"Blitz3:System/tokenlist.hashmap"}
    NPrint "Open file..."
  ;  fid.l = file_Open{"Sourcecodes:Amiblitz3/Sourcecodes/testab3.include.ab2",#file_read}
    fid.l = file_Open{"Sourcecodes:Amiblitz3/Sourcecodes/Amiblitz3/compiler/amiblitz3.ab2",#file_read}
    If fid>=0
      blength.l = file_GetLength{fid}
      ;blength.l = 25500
      tmp.l  = AllocMem(blength,#MEMF_CLEAR)
      tmp2.l = AllocMem(blength,#MEMF_CLEAR)
      tmp3.l = AllocMem(blength,#MEMF_CLEAR)
      NPrint "Read file..."
      file_ReadMem{fid,tmp,blength}
      NPrint "Tokenizing..."
      eclock_Start{1000}
      tl.l  = preproc_Tokenize{*tn,tmp,tmp2,blength}
      time.l = eclock_Stop{}
      NPrint "DONE!!! (",blength," => ",tl,"bytes) in ",time,"ms!"
      NPrint "Detokenizing..."
      eclock_Start{1000}
      trl.l = preproc_Detokenize{*tn,tmp2,tmp3,tl}
      time.l = eclock_Stop{}
      NPrint "DONE!!! (",tl," => ",trl,"bytes) in ",time,"ms!"

      file_Close{fid}
      If tl>0
        fid.l = file_Open{"ram:itest.bb2",#file_forcewrite}
        If fid>=0
          file_WriteMem{fid,tmp2,tl}
          file_Close{fid}
        End If
      Else
        NPrint "tokenizer error!"
      End If
      If trl>0
        fid.l = file_Open{"ram:itest.txt",#file_forcewrite}
        If fid>=0
          file_WriteMem{fid,tmp3,trl}
          file_Close{fid}
        End If
      Else
        NPrint "De-tokenizer error!"
      End If

    Else
      NPrint "Unable to open source file!"
    End If
  Else
    NPrint "Unable to load tokenizer!"
  End If
  preproc_Free{*tn}
Else
  NPrint "Unable to load tokenizer!"
End If
NPrint "end!"
End
CEND

